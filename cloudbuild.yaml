# cloudbuild.yaml
# Define substitution variables for project ID and GCS bucket for artifacts
substitutions:
  _GCP_PROJECT_ID: "capstoneproject-462618" # Your GCP Project ID
  _GCS_BUCKET_NAME: "capstone-nasa-wildfire-sal" # Your GCS bucket for data and model artifacts

steps:
  # Step 1: Build & push Docker image to Artifact Registry
  # Using $COMMIT_SHA for specific tagging, ensuring unique image for each build
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Build_Docker_Image'
    args:
      - 'build'
      - '-t'
      - 'us-central1-docker.pkg.dev/${_GCP_PROJECT_ID}/capstone/ml-model:$COMMIT_SHA'
      - '.'
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Push_Docker_Image'
    args:
      - 'push'
      - 'us-central1-docker.pkg.dev/${_GCP_PROJECT_ID}/capstone/ml-model:$COMMIT_SHA'

  # Step 2: Submit Vertex AI Custom Training Job
  # This job will use the Docker image built in the previous step
  # Pass the GCS bucket name as an argument to your training script
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Vertex_AI_Training'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        gcloud ai custom-jobs create \
          --region=us-central1 \
          --display-name=wildfire-train-$SHORT_SHA \
          --worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=us-central1-docker.pkg.dev/${_GCP_PROJECT_ID}/capstone/ml-model:$COMMIT_SHA \
          --args="--bucket=${_GCS_BUCKET_NAME}" \
          --project=${_GCP_PROJECT_ID} # Explicitly specify project

  # Step 3: Register Model in Vertex AI Model Registry
  # This step assumes your train.py uploads the model to gs://<your-bucket>/model/tf_model/saved_model.pb
  # And potentially other artifacts like scaler.pkl
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Register_Model'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        MODEL_DISPLAY_NAME="wildfire-detection-model"
        MODEL_ARTIFACT_URI="gs://${_GCS_BUCKET_NAME}/model/tf_model/" # Path to your SavedModel directory
        TRAINING_CONTAINER_IMAGE="us-central1-docker.pkg.dev/${_GCP_PROJECT_ID}/capstone/ml-model:$COMMIT_SHA" # The image used for training

        gcloud ai models upload \
          --region=us-central1 \
          --display-name=$MODEL_DISPLAY_NAME \
          --artifact-uri=$MODEL_ARTIFACT_URI \
          --container-predict-route="/predict" \
          --container-health-route="/health" \
          --container-ports=8080 \
          --container-image-uri=$TRAINING_CONTAINER_IMAGE \
          --project=${_GCP_PROJECT_ID}
        # The `--container-image-uri` here should ideally be a *serving* image,
        # which might be a pre-built Vertex AI serving image (e.g., tf2-cpu)
        # or a custom serving image you build separately.
        # For simplicity, I'm reusing the training image for now, but a dedicated
        # serving image is a better practice.
        # Ensure your Dockerfile's ENTRYPOINT is set to a serving command for deployment.
        # If your model needs a different serving image, you'll need another build step for it.

  # Step 4: (Optional) Deploy Model to Vertex AI Endpoint
  # This step creates an endpoint if it doesn't exist and deploys the latest model version.
  # This is for continuous *deployment* to a staging environment.
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Deploy_Model'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        MODEL_DISPLAY_NAME="wildfire-detection-model"
        ENDPOINT_DISPLAY_NAME="wildfire-prediction-endpoint"
        # Find the latest version of the model to deploy
        MODEL_ID=$(gcloud ai models list --region=us-central1 --filter="displayName=${MODEL_DISPLAY_NAME}" --format="value(name)" --limit=1)
        
        # Check if endpoint exists, create if not
        ENDPOINT_ID=$(gcloud ai endpoints list --region=us-central1 --filter="displayName=${ENDPOINT_DISPLAY_NAME}" --format="value(name)" --limit=1)
        if [ -z "$ENDPOINT_ID" ]; then
          echo "Endpoint not found, creating a new one..."
          ENDPOINT_ID=$(gcloud ai endpoints create \
            --region=us-central1 \
            --display-name=$ENDPOINT_DISPLAY_NAME \
            --project=${_GCP_PROJECT_ID} \
            --format="value(name)")
          echo "Created endpoint: $ENDPOINT_ID"
        else
          echo "Using existing endpoint: $ENDPOINT_ID"
        fi

        # Deploy the model to the endpoint
        gcloud ai endpoints deploy-model $ENDPOINT_ID \
          --region=us-central1 \
          --model=$MODEL_ID \
          --display-name="deployed-wildfire-model-$SHORT_SHA" \
          --machine-type=n1-standard-2 \
          --min-replica-count=1 \
          --max-replica-count=1 \
          --traffic-split=0=100 \
          --project=${_GCP_PROJECT_ID}
        # You might need to adjust machine-type, min/max-replica-count based on your needs.
        # traffic-split=0=100 means 100% of traffic goes to this newly deployed model.
        # For A/B testing or canary deployments, you'd adjust traffic-split.

images:
  - 'us-central1-docker.pkg.dev/${_GCP_PROJECT_ID}/capstone/ml-model:$COMMIT_SHA'

options:
  logging: CLOUD_LOGGING_ONLY